---
title: 机器学习（三）：绪论
date: 2025-05-26 23:10:02
tags:
  - 机器学习
  - Machine Learning
  - 高斯分布
  - 边缘概率分布
  - 条件概率分布
  - 线性高斯模型
  - 频率派
  - 贝叶斯派
categories:
  - AI
---


# 基本的数学知识

这一部分笔记主要学习高斯分布的联合概率分布、边缘概率和条件概率分布的求解过程。

## 高斯分布的边缘概率分布和条件概率分布
### 线性高斯模型

[《机器学习（二）：绪论》](https://tao-oooo.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%BB%AA%E8%AE%BA/index.html)笔记讲的是已知联合概率分布求边缘概率分布和条件概率分布。这里是给定部分随机变量的边缘概率密度和条件概率密度求联合概率密度以及其他随机变量的边缘概率密度和条件概率密度。

给定一个高斯边缘分布$p(x)$和一个高斯条件分布$p(y|x)$，其中$p(y|x)$的均值是$x$的线性函数，协方差与$x$无关，这样的问题被称为线性高斯模型。用数学的语言来表达就是：

已知边缘概率分布和条件概率分布
$$
\begin{equation}
    \begin{aligned}
        &p(x) = N(x|\mu, \Lambda^{-1}) \\
        &p(y|x) = N(y|A x + b, L^{-1})
    \end{aligned}
\end{equation}
$$

其中，$\Lambda, L^{-1}$为协方差矩阵，它们的逆矩阵$\Lambda, L $可以被称为精度矩阵。

希望求出随机变量$y$的边缘概率分布$p(y)$和条件概率分布$p(x|y)$。

#### 随机变量y的分布

我们先求$y$的分布，由式(1)可以知道$y$满足如下表达式：
$$
\begin{equation}
    y = A x + b + \varepsilon
\end{equation}
$$

其中，$\varepsilon \sim N(0, L^{-1})$且$x$与$\varepsilon$相互独立，即$x \perp \varepsilon$。

!!! Note 为什么$y = A x + b + \varepsilon$呢？
    给定$x$时，$y|x$的期望是：
    $$
    \begin{equation} \notag
        \mathbb{E}[y|x] = \mathbb{E}[A x + b + \varepsilon] = \mathbb{E}[A x + b] + \mathbb{E}[\varepsilon] = A x + b
    \end{equation}
    $$

    $y|x$的方差是：
    $$
    \begin{equation} \notag
        \mathbb{D}[y|x] = \mathbb{D}[A x + b + \varepsilon] = \mathbb{D}[A x + b] + \mathbb{D}[\varepsilon] = L^{-1}
    \end{equation}
    $$

    其中，因为$x$是条件，所以$A x + b$是常量，则$\mathbb{E}[A x + b] = A x + b$，$\mathbb{D}[A x + b] = 0$。

    最终可以发现，$y|x$的分布就是式(1)定义的分布。

于是，根据式(2)，可以直接求出$y$的均值为：
$$
\begin{equation}
    \begin{aligned}
        \mathbb{E}[y] &= \mathbb{E}[A x + b + \varepsilon] \\
        &= \textcolor{red}{\mathbb{E}[A x + b]} + \mathbb{E}[\varepsilon] \\
        &= A \mu + b
    \end{aligned}
\end{equation}
$$

也可以求出$y$的方差为：
$$
\begin{equation}
    \begin{aligned}
        \mathbb{D}[y] &= \mathbb{D}[A x + b + \varepsilon] \\
        &= \textcolor{blue}{\mathbb{D}[A x + b]} + \mathbb{D}[\varepsilon] \\
        &= L^{-1} + A \Lambda^{-1} A^{\top}
    \end{aligned}
\end{equation}
$$

这里不同于$y|x$的情形，$x$在这里是随机变量，所以$A x + b$也是随机变量，因此$\textcolor{red}{\mathbb{E}[A x + b] = A \mu + b}$，$\textcolor{blue}{\mathbb{D}[A x + b] = L^{-1}}$。

最终可得$y$服从如下分布：
$$
\begin{equation}
    \boxed{y \sim N(A \mu + b, L^{-1} + A \Lambda^{-1} A^{\top})}
\end{equation}
$$

#### 条件随机变量x|y的分布

接下来求$y|x$的分布，直接求条件概率分布比较麻烦。这里需要使用[《机器学习（二）：绪论》](https://tao-oooo.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%BB%AA%E8%AE%BA/index.html)笔记中的式(23)或式(24)的公式直接求解，因此需要先构造$x$和$y$的联合概率分布。

使用$x$和$y$构造随机变量$z$：
$$
\begin{equation} \notag
    z = \begin{bmatrix} x \\ y \end{bmatrix}
\end{equation}
$$

根据边缘概率分布的规律可以得到$z$的期望为：
$$
\begin{equation}
    \mathbb{E}[z] = \begin{bmatrix} \mu \\ A \mu + b \end{bmatrix}
\end{equation}
$$

接着根据$\mathbb{D}[x] = \Lambda^{-1}$、$\mathbb{E}[x] = \mu$以及式(3)中的$\mathbb{E}[y]$可以得到$x$与$y$之间的协方差矩阵：
$$
\begin{equation}
    \begin{aligned}
        \text{cov}(x, y) &= \mathbb{E}[(x - \mathbb{E}[x])(y - \mathbb{E}[y])^{\top}] \\
        &= \mathbb{E}[(x - \mu)(A x + b + \varepsilon - A \mu - b)^{\top}] \\
        &= \mathbb{E}[(x - \mu)(A x - A \mu + \varepsilon)^{\top}] \\
        &= \mathbb{E}[(x - \mu)(A x - A \mu)^{\top}] + \mathbb{E}[(x - \mu) \varepsilon^{\top}] \\
        &= \mathbb{E}[(x - \mu)(x - \mu)^{\top} A^{\top}] \\
        &= \mathbb{D}[x] A^{\top} \\
        &= \Lambda^{-1} A^{\top}
    \end{aligned}
\end{equation}
$$

其中，由于$x \perp \varepsilon$，所以$(x - \mu) \perp \varepsilon$，则$\mathbb{E}[(x - \mu) \varepsilon^{\top}] = \mathbb{E}[x - \mu] \mathbb{E}[\varepsilon^{\top}] = 0$。

由已知的$x$分布、式(6)和式(7)以及边缘概率分布的求解规律，可以得到随机变量$z$的分布：
$$
\begin{equation}
    \boxed{z =
    \begin{bmatrix}
        x \\ y
    \end{bmatrix} \sim N
    \left(
        \begin{bmatrix}
            \mu \\ A \mu + b
        \end{bmatrix},
        \begin{bmatrix}
            \Sigma_{xx} & \Sigma_{xy} \\
            \Sigma_{yx} & \Sigma_{yy}
        \end{bmatrix}
    \right)}
\end{equation}
$$

代入协方差矩阵后即为即
$$
\begin{equation}
    z \sim N
    \left(
        \begin{bmatrix}
            \mu \\ A \mu + b
        \end{bmatrix},
        \begin{bmatrix}
            \Lambda^{-1} & \Lambda^{-1} A^{\top} \\
            A \Lambda^{-1} & L^{-1} + A \Lambda^{-1} A^{\top}
        \end{bmatrix}
    \right)
\end{equation}
$$

再由条件概率分布求解公式就可以得到条件随机变量$x|y$的分布：
$$
\begin{equation}
    \boxed{x|y \sim N
    \left(
        \mu_x + \Sigma_{xy} \Sigma_{yy}^{-1} (y - \mu_y),
        \Sigma_{xx} - \Sigma_{xy} \Sigma_{yy}^{-1} \Sigma_{yx}
    \right)}
\end{equation}
$$

代入协方差矩阵后即为：
$$
\begin{equation}
    \begin{aligned}
        x|y \sim N
        &\left(
            \mu + \Lambda^{-1} A^{\top} (L^{-1} + A \Lambda^{-1} A^{\top})^{-1} (y - A \mu - b),
        \right. \\
        &\left.
            \Lambda^{-1} - \Lambda^{-1} A^{\top} (L^{-1} + A \Lambda^{-1} A^{\top})^{-1} \Lambda^{-1} A^{\top}
        \right)
    \end{aligned}
\end{equation}
$$


# 机器学习

机器学习领域认为，数据是一个概率模型，所以统计相关的一些技术被引入到机器学习中。

机器学习大致可以分为两个派别：频率派和贝叶斯派。

* 对于频率派而言，参数$\theta$被认为是未知常量，数据$\boldsymbol{X}$为随机变量。我们的目的是通过数据$\boldsymbol{X}$将参数$\theta$估计出来，常用的估计方法是极大似然估计(MLP)。
* 对于贝叶斯派而言，参数$\theta$也被认为是随机变量，并且$\theta \sim p(\theta)$，这里$p(\theta)$被称为先验。在实际建模的过程中，通过贝叶斯公式将先验与似然联系起来，即
$$
\begin{equation}
    \underbrace{P(\theta | \boldsymbol{X})}_{\text{Posterior}} = \frac{\overbrace{P(\boldsymbol{X} | \theta)}^{\text{Likelihood}} \overbrace{P(\theta)}^{\text{Prior}}}{\underbrace{P(\boldsymbol{X})}_{\int_{\theta} P(\boldsymbol{X} | \theta) P(\theta) \text{d} \theta}}
\end{equation}
$$

# 参考文献

[1]Bilibili《白板推导机器学习》系列

[2]C. M. Bishop. 《Pattern recognition and machine learning》