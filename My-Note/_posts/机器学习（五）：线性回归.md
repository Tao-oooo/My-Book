---
title: 机器学习（五）：线性回归
tags:
---

# 线性回归
## 2. 正则化

### Loss Function:

\[ L(W) = \sum_{i=1}^N \| W x_i - y_i \|_2^2 \]

\[ \hat{W} = (X^T X)^{-1} X^T Y \]

\[ X_{N \times P}, \quad N \text{ 个样本}, \quad x \in \mathbb{R}^P \]

### 过拟合：

1. 加入数据
2. 特征选择/特征提取（PCA）
3. 正则化

### 正则化框架：

\[ \arg\min_W \left[ L(W) + \lambda P(W) \right] \]

- **L1**: Lasso, \( l_1 \) 范数, \( P(W) = \| W \|_1 \)
- **L2**: Ridge, \( l_2 \) 范数, 岭回归, \( P(W) = \| W \|_2^2 = W^T W \)

\[ \Rightarrow \text{weight decay 权值衰减} \]

# (1) 频率角度

\[ J(W) = L(W) + \lambda P(W) \]

\[ = \sum_{i=1}^N \| W^T x_i - y_i \|^2 + \lambda \| W \|^2 \]

\[ = \sum_{i=1}^N (W^T x_i - y_i)(W^T x_i - y_i)^T + \lambda W^T W \]

\[ = (W X^T - Y^T)(X W - Y) + \lambda W^T W \]

\[ = W^T X^T X W - 2 W^T X^T Y + Y^T Y + \lambda W^T W \]

\[ = W^T (X^T X + \lambda I) W - 2 W^T X^T Y + Y^T Y \]

\[ \hat{W} = \arg\min_W J(W) \]

\[ \frac{\partial J(W)}{\partial W} = 2 (X^T X + \lambda I) W - 2 X^T Y = 0 \]

\[ \Rightarrow \hat{W} = (X^T X + \lambda I)^{-1} X^T Y \]

\(X^T X\) 不一定可逆，\(X^T X\) 为半正定，\(X^T X + \lambda I\) 一定可逆，有效抑制了过拟合。

---

# (2) 贝叶斯角度

**先验**：

\[ W \sim \mathcal{N}(0, \sigma_0^2), \quad \varepsilon \sim \mathcal{N}(0, \sigma^2) \]

\[ P(W) = \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left\{ -\frac{\| W \|^2}{2\sigma_0^2} \right\} \]

\[ P(Y | X, W) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{ -\frac{(Y - W^T X)^2}{2\sigma^2} \right\} \]

**最大后验概率（MAP）**：

\[ \hat{W} = \arg\max_W P(W | Y) = \arg\max_W P(Y | X, W) P(W) \]

\[ = \arg\max_W \log \left[ P(Y | X, W) P(W) \right] \]

\[ = \arg\max_W \log \left[ \frac{1}{\sqrt{2\pi}\sigma} \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left\{ -\frac{(Y - W^T X)^2}{2\sigma^2} - \frac{\| W \|^2}{2\sigma_0^2} \right\} \right] \]

\[ = \arg\max_W \left[ -\frac{(Y - W^T X)^2}{2\sigma^2} - \frac{\| W \|^2}{2\sigma_0^2} \right] \]

\[ = \arg\min_W \left[ \frac{(Y - W^T X)^2}{2\sigma^2} + \frac{\| W \|^2}{2\sigma_0^2} \right] \]

\[ = \arg\min_W \left( \sum_{i=1}^N \| y_i - W^T x_i \|^2 + \frac{\sigma^2}{\sigma_0^2} \| W \|^2 \right) \]

因此：

Regularized LSE \(\Leftrightarrow\) MAP (noise is GD, prior is GD.)

---

# 参考文献

[1]Bilibili《白板推导机器学习》系列

[2]C. M. Bishop. 《Pattern recognition and machine learning》

