---
title: 机器学习（一）：绪论
tags:
  - 机器学习
  - Machine Learning
  - 高斯分布
  - 极大似然估计
categories:
  - AI
date: 2025-05-14 22:51:25
---


# 基本的数学知识
## 高斯分布(Gaussian Distribution)

在机器学习中，通常情况下我们都会假设数据服从高斯分布（正态分布），所以它是一个非常重要的分布，这一部分回顾了一元高斯分布和多元高斯分布的公式。

一维高斯分布表达式如下：
$$
\begin{equation}
    p(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp(-\frac{(x - \mu)^2}{2\sigma^2})
\end{equation}
$$
其中，$\mu$是均值，$\sigma$是方差。

$N$维高斯分布表达式如下：
$$
\begin{equation}
    p(\boldsymbol{X}) = \frac{1}{(2\pi)^{\frac{N}{2}} |\Sigma|^{\frac{1}{2}}} \exp(-\frac{1}{2} (\boldsymbol{X} - \mu)^{\top} \Sigma^{-1} (\boldsymbol{X} - \mu))
\end{equation}
$$
其中，$\mu$是$(N \times 1)的$均值向量，$\Sigma$是$(N \times N)$的协方差矩阵。

## 极大似然估计(Maximum Likelihood Estimation, MLE)

极大似然估计(Maximum Likelihood Estimation, MLE)是一种常用的参数估计方法。它的核心思想是：在已知观测数据的前提下，找到最有可能生成这些数据的参数值。

假设我们有观测数据集
$$
\boldsymbol{X} = (\boldsymbol{x}_1, \boldsymbol{x}_2, \cdots, \boldsymbol{x_N})^{\top} = \begin{bmatrix} \boldsymbol{x}_1^{\top} \\ \boldsymbol{x}_2^{\top} \\ \vdots \\ \boldsymbol{x}_N^{\top} \end{bmatrix}_{(N \times p)}, \boldsymbol{x}_i \in \mathbb{R}^p, \boldsymbol{x}_i \sim N(\mu, \Sigma)
$$

我们希望找到观测数据的分布，即需要估计参数$\theta = (\mu, \Sigma)$。

此时使用极大似然估计对参数$\theta$进行估计，即优化目标为：
$$
\begin{equation}
    \theta_{\text{MLE}} = \text{arg max}_{\theta} P(\boldsymbol{X} | \theta)
\end{equation}
$$

我们以一元高斯分布为例，此时$\theta = (\mu, \sigma^2)$，对数似然函数如下：
$$
\begin{equation}
    \begin{aligned}
        \log P(\boldsymbol{X} | \theta) &= \log \prod_{i = 1}^N p(x_i | \theta) \\
        &= \sum_{i = 1}^N \log p(x_i | \theta) \\
        &= \sum_{i = 1}^N \left[ \log \frac{1}{\sqrt{2\pi}} + \log \frac{1}{\sigma} - \frac{(x_i - \mu)^2}{2\sigma^2} \right]
    \end{aligned}
\end{equation}
$$

### 对均值的估计

我们先对均值$\mu$进行估计：

$$
\begin{equation}
    \begin{aligned}
        \mu_{\text{MLE}} &= \text{arg max}_{\mu} \sum_{i = 1}^N \left[ \underbrace{\textcolor{red}{\log \frac{1}{\sqrt{2\pi}} + \log \frac{1}{\sigma}}}_{\text{此项与$\mu$无关}} - \frac{(x_i - \mu)^2}{2\sigma^2} \right] \\
        &= \text{arg max}_{\mu} \sum_{i = 1}^N \underbrace{\left[ - \frac{(x_i - \mu)^2}{ {\textcolor{red}{2\sigma^2} }} \right]}_{\text{分母也与$\mu$无关}} \\
        &= \text{arg max}_{\mu} \sum_{i = 1}^N \left[ - (x_i - \mu)^2 \right]
    \end{aligned}
\end{equation}
$$

令$\frac{\partial}{\partial \mu} \sum_{i = 1}^N \left[ - (x_i - \mu)^2 \right] = \sum_{i = 0}^N (2(x_i - \mu)) = 0$可得：
$$
\begin{equation}
    \boxed{\mu_{\text{MLE}} = \frac{1}{N} \sum_{i = 1}^N x_i}
\end{equation}
$$

可以验证这是一个 **无偏估计** ，因为根据无偏估计的定义可以得到：
$$
\begin{equation}
    \mathbb{E} [\mu_{\text{MLE}}] = \mathbb{E} \left[ \frac{1}{N} \sum_{i = 1}^N x_i \right] = \frac{1}{N} \sum_{i = 1}^N \mathbb{E} [x_i] = \mu
\end{equation}
$$

### 对方差的估计

接着使用同样的方法对方差$\sigma^2$进行估计：
$$
\begin{equation}
    \begin{aligned}
        \sigma^2_{\text{MLE}} &= \text{arg max}_{\sigma} \sum_{i = 1}^N \left[ \underbrace{\textcolor{red}{\log \frac{1}{\sqrt{2\pi}}}}_{\text{此项与$\sigma$无关}} + \log \frac{1}{\sigma} - \frac{(x_i - \mu)^2}{2\sigma^2} \right] \\
        &= \text{arg max}_{\sigma} \sum_{i = 1}^N \left[ \log \frac{1}{\sigma} - \frac{(x_i - \mu)^2}{2\sigma^2} \right]
    \end{aligned}
\end{equation}
$$

令$\frac{\partial}{\partial \sigma} \sum_{i = 1}^N \left[ \log \frac{1}{\sigma} - \frac{(x_i - \mu)^2}{2\sigma^2} \right] = \sum_{i = 1}^N \left[ -\frac{1}{\sigma} + \frac{(x_i - \mu)^2}{\sigma^3} \right] = 0$可得：
$$
\begin{equation}
    \sigma^2_{\text{MLE}} = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2
\end{equation}
$$

因为式(9)中仍然带有待估计的参数$\mu$，所以可以将已经估计出的$\mu_{\text{MLE}}$代入，如下所示：
$$
\begin{equation} \notag
    \begin{aligned}
        \sigma^2_{\text{MLE}} &= \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2 \\
        &= \frac{1}{N} \sum_{i = 1}^N (x_i - \mu_{\text{MLE}})^2 \\
        &= \frac{1}{N} \sum_{i = 1}^N \left[ x_i^2 - 2 x_i \mu_{\text{MLE}} + \mu_{\text{MLE}}^2 \right] \\
        &= \frac{1}{N} \sum_{i = 1}^N x_i^2 - \underbrace{\textcolor{red}{\frac{1}{N} \sum_{i = 1}^N} (2 \textcolor{red}{x_i} \mu_{\text{MLE}})}_{\text{红色部分就是$\mu_{\text{MLE}}$}} + \frac{1}{N} \sum_{i = 1}^N \mu_{\text{MLE}}^2 \\
        &= \frac{1}{N} \sum_{i = 1}^N x_i^2 - 2\mu_{\text{MLE}}^2 + \mu_{\text{MLE}}^2 \\
        &= \frac{1}{N} \sum_{i = 1}^N x_i^2 - \mu_{\text{MLE}}^2
    \end{aligned}
\end{equation}
$$

那么最终估计的$\sigma^2_{\text{MLE}}$即为：
$$
\begin{equation}
    \boxed{\sigma^2_{\text{MLE}} = \frac{1}{N} \sum_{i = 1}^N x_i^2 - \mu_{\text{MLE}}^2}
\end{equation}
$$

这里与$\mu_{\text{MLE}}$不一样的地方是$\sigma^2$的估计是一个 **有偏估计** ，因为可以进一步推导发现$\mathbb{E}[\sigma^2_{\text{MLE}}] \neq \sigma^2$，详细推导如下：
$$
\begin{equation}
    \begin{aligned}
        \mathbb{E} [\sigma^2_{\text{MLE}}] &= \mathbb{E} \left[ \frac{1}{N} \sum_{i = 1}^N x_i^2 - \mu_{\text{MLE}}^2 \right] \\
        &= \frac{1}{N} \sum_{i = 1}^N \mathbb{E}[x_i^2] - \mathbb{E}[\mu_{\text{MLE}}^2] \\
        &= \frac{1}{N} \sum_{i = 1}^N \textcolor{blue}{\mathbb{E}[x_i^2]} - (\textcolor{red}{\mathbb{D} [\mu_{\text{MLE}}]} + \mathbb{E}^2 [\mu_{\text{MLE}}])
    \end{aligned}
\end{equation}
$$

接下来为了进一步化简，这里需要用到期望与方差的关系(来自期望方差的定义)：

$$
\begin{equation}
    \mathbb{D}[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2
\end{equation}
$$

先求$\textcolor{blue}{\mathbb{E}[x_i^2]}$这一部分，由式(12)可以得到：

$$
\begin{equation}
    \mathbb{E}[x_i^2] = \mathbb{E}[x_i]^2 + \mathbb{D}[x_i] = \mu^2 + \sigma^2
\end{equation}
$$

接下来求$\textcolor{red}{\mathbb{D}[\mu_{\text{MLE}}]}$这部分，根据式(6)中的$\mu_{\text{MLE}}$可得：

$$
\begin{equation}
    \mathbb{D}[\mu_{\text{MLE}}] = \mathbb{D}\left[\frac{1}{N} \sum_{i=1}^N x_i\right] = \frac{1}{N^2} \mathbb{D}[x_i] = \frac{\sigma^2}{N}
\end{equation}
$$

最后，由式(7)可知$\mathbb{E}[\mu_{\text{MLE}}] = \mu$。

于是，式(11)可以继续化简：

$$
\begin{equation}
    \begin{aligned}
        \mathbb{E}[\sigma^2_{\text{MLE}}] &= \frac{1}{N} \sum_{i = 1}^N \textcolor{blue}{\mathbb{E}[x_i^2]} - (\textcolor{red}{\mathbb{D} [\mu_{\text{MLE}}]} + \mathbb{E}^2 [\mu_{\text{MLE}}]) \\
        &= \frac{1}{N} \sum_{i=1}^N \left( \mu^2 + \sigma^2 \right) - \left( \frac{\sigma^2}{N} + \mu^2 \right) \\
        &= \mu^2 + \sigma^2 - \left( \frac{\sigma^2}{N} + \mu^2 \right) \\
        &= \sigma^2 - \frac{\sigma^2}{N} \\
        &= \frac{N - 1}{N} \sigma^2 \\
        &\neq \sigma^2
    \end{aligned}
\end{equation}
$$

当然，根据无偏估计的定义可以推导出，只有当$\sigma_{\text{MLE}}^2 = \frac{1}{N - 1} \sum_{i = 1}^N (x_i - \mu_{\text{MLE}})^2$时，$\sigma_{\text{MLE}}^2$才是无偏的。



# 参考文献

[1]Bilibili《白板推导机器学习》系列

[2]C. M. Bishop. 《Pattern recognition and machine learning》
