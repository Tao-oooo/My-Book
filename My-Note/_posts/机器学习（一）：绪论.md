---
title: 机器学习（一）：绪论
tags:
---

# 基本的数学知识
## 高斯分布(Gaussian Distribution)

在机器学习中，通常情况下我们都会假设数据服从高斯分布(正态分布)，所以它是一个非常重要的分布，这一部分回顾了一元高斯分布和多元高斯分布的公式。
一维高斯分布表达式如下：
$$
\begin{equation}
    p(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp(-\frac{(x - \mu)^2}{2\sigma^2})
\end{equation}
$$
其中，$\mu$是均值，$\sigma$是方差。

$N$维高斯分布表达式如下：
$$
\begin{equation}
    p(\boldsymbol{X}) = \frac{1}{(2\pi)^{\frac{N}{2}} |\Sigma|^{\frac{1}{2}}} \exp(-\frac{1}{2} (\boldsymbol{X} - \mu)^{\top} \Sigma^{-1} (\boldsymbol{X} - \mu))
\end{equation}
$$
其中，$\mu$是均值向量，$\Sigma$是协方差矩阵。

## 极大似然估计(Maximum Likelihood Estimation, MLE)

极大似然估计(Maximum Likelihood Estimation, MLE)是一种常用的参数估计方法，其核心思想是：在已知观测数据的前提下，找到最有可能生成这些数据的参数值。

假设数据集为$\boldsymbol{X} = (\boldsymbol{x}_1, \boldsymbol{x}_2, \cdots, \boldsymbol{x_N})^{\top} = \begin{bmatrix} \boldsymbol{x}_1^{\top} \\ \boldsymbol{x}_2^{\top} \\ \vdots \\ \boldsymbol{x}_N^{\top} \end{bmatrix}_{(N \times p)}, \boldsymbol{x}_i \in \mathbb{R}^p, \boldsymbol{x}_i \sim N(\mu, \Sigma)$，我们希望找到观察数据的分布，就需要找到$\theta = (\mu, \Sigma)$，也就是需要估计参数$\theta$。

此时使用极大似然估计对参数$\theta$进行估计，即：
$$
\begin{equation}
    \theta_{\text{MLE}} = \text{arg max}_{\theta} P(\boldsymbol{X} | \theta)
\end{equation}
$$

当$p = 1$时，$\theta = (\mu, \sigma^2)$，
$$
\begin{equation}
    \begin{aligned}
        \log P(\boldsymbol{X} | \theta) &= \log \prod_{i = 1}^N p(x_i | \theta) \\
        &= \sum_{i = 1}^N \log p(x_i | \theta) \\
        &= \sum_{i = 1}^N \left[ \log \frac{1}{\sqrt{2\pi}} + \log \frac{1}{\sigma} - \frac{(x_i - \mu)^2}{2\sigma^2} \right]
    \end{aligned}
\end{equation}
$$

使用MLE对均值$\mu$进行估计：

$$
\begin{equation}
    \begin{aligned}
        \mu_{\text{MLE}} &= \text{arg max}_{\mu} \sum_{i = 1}^N \left[ \underbrace{\textcolor{red}{\log \frac{1}{\sqrt{2\pi}} + \log \frac{1}{\sigma}}}_{\text{此项与$\mu$无关}} - \frac{(x_i - \mu)^2}{2\sigma^2} \right] \\
        &= \text{arg max}_{\mu} \sum_{i = 1}^N \underbrace{\left[ - \frac{(x_i - \mu)^2}{ {\textcolor{red}{2\sigma^2} }} \right]}_{\text{分母也与$\mu$无关}} \\
        &= \text{arg max}_{\mu} \sum_{i = 1}^N \left[ - (x_i - \mu)^2 \right]
    \end{aligned}
\end{equation}
$$

令$\frac{\partial}{\partial \mu} \sum_{i = 1}^N \left[ - (x_i - \mu)^2 \right] = \sum_{i = 0}^N (2(x_i - \mu)) = 0$可得：
$$
\begin{equation}
    \mu_{\text{MLE}} = \frac{1}{N} \sum_{i = 1}^N x_i
\end{equation}
$$
这是一个 **无偏估计** ，因为：
$$
\begin{equation}
    \mathbb{E} [\mu_{\text{MLE}}] = \mathbb{E} \left[ \frac{1}{N} \sum_{i = 1}^N x_i \right] = \frac{1}{N} \sum_{i = 1}^N \mathbb{E} [x_i] = \mu
\end{equation}
$$

使用MLE对方差$\sigma^2$进行估计：
$$
\begin{equation}
    \begin{aligned}
        \sigma^2_{\text{MLE}} &= \text{arg max}_{\sigma} \sum_{i = 1}^N \left[ \underbrace{\textcolor{red}{\log \frac{1}{\sqrt{2\pi}}}}_{\text{此项与$\sigma$无关}} + \log \frac{1}{\sigma} - \frac{(x_i - \mu)^2}{2\sigma^2} \right] \\
        &= \text{arg max}_{\sigma} \sum_{i = 1}^N \left[ \log \frac{1}{\sigma} - \frac{(x_i - \mu)^2}{2\sigma^2} \right]
    \end{aligned}
\end{equation}
$$
令$\frac{\partial}{\partial \sigma} \sum_{i = 1}^N \left[ \log \frac{1}{\sigma} - \frac{(x_i - \mu)^2}{2\sigma^2} \right] = \sum_{i = 1}^N \left[ -\frac{1}{\sigma} + \frac{(x_i - \mu)^2}{\sigma^3} \right] = 0$可得：
$$
\begin{equation}
    \sigma^2_{\text{MLE}} = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2
\end{equation}
$$

如果将$\mu_{\text{MLE}}$代入(9)式：
$$
\begin{equation}
    \begin{aligned}
        \sigma^2_{\text{MLE}} &= \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2 \\
        &= \frac{1}{N} \sum_{i = 1}^N (x_i - \mu_{\text{MLE}})^2 \\
        &= \frac{1}{N} \sum_{i = 1}^N \left[ x_i^2 - 2 x_i \mu_{\text{MLE}} + \mu_{\text{MLE}}^2 \right] \\
        &= \frac{1}{N} \sum_{i = 1}^N x_i^2 - \underbrace{\textcolor{red}{\frac{1}{N} \sum_{i = 1}^N} (2 \textcolor{red}{x_i} \mu_{\text{MLE}})}_{\text{红色部分就是$\mu_{\text{MLE}}$}} + \frac{1}{N} \sum_{i = 1}^N \mu_{\text{MLE}}^2 \\
        &= \frac{1}{N} \sum_{i = 1}^N x_i^2 - 2\mu_{\text{MLE}}^2 + \mu_{\text{MLE}}^2 \\
        &= \frac{1}{N} \sum_{i = 1}^N x_i^2 - \mu_{\text{MLE}}^2
    \end{aligned}
\end{equation}
$$
这是一个 **有偏估计** ，因为：
$$
\begin{equation}
    \begin{aligned}
        \mathbb{E} [\sigma^2_{\text{MLE}}] &= \mathbb{E} \left[ \frac{1}{N} \sum_{i = 1}^N x_i^2 - \mu_{\text{MLE}}^2 \right] \\
        &= \frac{1}{N} \sum_{i = 1}^N \mathbb{E}[x_i^2] - \mathbb{E}[\mu_{\text{MLE}}^2] \\
        &= \frac{1}{N} \sum_{i = 1}^N \mathbb{E}[x_i^2] - (\mathbb{D} [\mu_{\text{MLE}}] + \mathbb{E}^2 [\mu_{\text{MLE}}])
    \end{aligned}
\end{equation}
$$
因为$\mathbb{E}[x_i] = \mu, \mathbb{D}[x_i] = \sigma^2$，根据期望与方差的关系($\mathbb{D}[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$)可得$\mathbb{E}[x_i^2] = \mu^2 + \sigma^2$。同时，$\mathbb{D}[\mu_{\text{MLE}}] = \mathbb{D}[\frac{1}{N} \sum_{i = 1}^N x_i] = \frac{1}{N^2} \mathbb{D}[x_i] = \frac{\sigma^2}{N}$，$\mathbb{E} [\mu_{\text{MLE}}] = \mu$。于是，式(11)可以继续化简：
$$
\begin{equation}
    \begin{aligned}
        \mathbb{E} [\sigma^2_{\text{MLE}}] &= \mu^2 + \sigma^2 - \left( \frac{\sigma^2}{N} + \mu^2 \right) \\
        &= \frac{N - 1}{N} \sigma^2 \neq \sigma^2
    \end{aligned}
\end{equation}
$$
只有当$\sigma_{\text{MLE}}^2 = \frac{1}{N - 1} \sum_{i = 1}^N (x_i - \mu_{\text{MLE}})^2$时，$\sigma_{\text{MLE}^2}$才是无偏的。

# 机器学习

机器学习领域认为，数据是一个概率模型，所以统计相关的一些技术被引入到机器学习中。

机器学习大致可以分为两个派别：频率派和贝叶斯派。

对于频率派而言，参数$\theta$被认为是未知常量，数据$\boldsymbol{X}$为随机变量。我们的目的是通过数据$\boldsymbol{X}$将参数$\theta$估计出来，常用的估计方法是极大似然估计。
对于贝叶斯派而言，参数$\theta$也被认为是随机变量，并且$\theta \sim p(\theta)$，这里$p(\theta)$被称为先验。在实际建模的过程中，通过贝叶斯公式将先验与似然联系起来，即
$$
\begin{equation}
    \underbrace{P(\theta | \boldsymbol{X})}_{\text{Posterior}} = \frac{\overbrace{P(\boldsymbol{X} | \theta)}^{\text{Likelihood}} \overbrace{P(\theta)}^{\text{Prior}}}{\underbrace{P(\boldsymbol{X})}_{\int_{\theta} P(\boldsymbol{X} | \theta) P(\theta) \text{d} \theta}}
\end{equation}
$$
