---
title: 机器学习（四）：线性回归
tags:
---

# 线性回归(Linear Regression)

维基百科中，线性回归的定义如下：

!!! note 线性回归
    在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。

## 最小二乘估计(Least Squares Estimation, LSE)

有$N$个数据组成数据集$D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$，其中$x_i \in \mathbb{R}^p, y_i \in \mathbb{R}, i = 1, 2, ⋯, N$，用矩阵的形式表述数据集就是:
$$
\begin{equation}
    X = (x_1, x_2, ⋯, x_N)^\top =
    \begin{bmatrix}
        x_{11} & x_{12} & \cdots & x_{1p} \\
        x_{21} & x_{22} & \cdots & x_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{N1} & x_{N2} & \cdots & x_{Np}
    \end{bmatrix}_{N \times p},
    Y =
    \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_N
    \end{bmatrix}_{N×1}
\end{equation}
$$

根据上图所示，可以将需要拟合的直线表示为：
$$
\begin{equation}
    f(W) = W^\top x,
    W = \begin{bmatrix}
        w_1 \\ w_2 \\ \vdots \\ w_p
    \end{bmatrix}
\end{equation}
$$

!!! note 偏置(Bias)
    通常来说，式(2)中还包含了偏置项$b$，即
    $$
    \begin{equation} \notag
        f(W) = W^\top X + b
    \end{equation}
    $$
    这是一个常数项，但通常为了方便表示，会将偏置项$b$省略。

为什么可以将偏置项省略？可以看如下推导过程：
$$
\begin{equation}
    \begin{aligned}
        f(W) &= W^\top x + \textcolor{red}{b} \\
        &= \begin{bmatrix}
            w_1 & w_2 & \cdots & w_p
        \end{bmatrix}
        \begin{bmatrix}
            x_1 \\ x_2 \\ \vdots \\ x_p
        \end{bmatrix} + \textcolor{red}{b} \\
        &= w_1 x_1 + w_2 x_2 + \cdots + w_p x_p + \textcolor{red}{b} \\
        &= w_1 x_1 + w_2 x_2 + \cdots + w_p x_p + \textcolor{red}{w_0 \times 1} \Longrightarrow \text{将$b$表示为$w_0 \times 1$} \\
        &= \begin{bmatrix}
            w_1 & w_2 & \cdots & w_p & \textcolor{red}{w_0}
        \end{bmatrix}
        \begin{bmatrix}
            x_1 \\ x_2 \\ ⋮ \\ x_p \\ \textcolor{red}{1}
        \end{bmatrix} \\
        &= (W^\prime)^\top X^\prime
    \end{aligned}
\end{equation}
$$

最小二乘法中会定义一个目标函数，被称为损失函数(Loss Function)，公式如下：
$$
\begin{equation}
    \begin{aligned}
        L(W) &= \sum_{i=1}^N \| W^\top x_i - y_i \|_2^2 \\
        &= \sum_{i=1}^N (W^\top x_i - y_i)^2 \\
        &= \begin{bmatrix}
            W^\top x_1 - y_1 & W^\top x_2 - y_2 & \cdots & W^\top x_N - y_N
        \end{bmatrix}
        \underbrace{\begin{bmatrix}
            W^\top x_1 - y_1 \\ W^\top x_2 - y_2 \\ \vdots \\ W^\top x_N - y_N
        \end{bmatrix}}_{\text{$=$左边项的转置}} \\
        &= \left(
            W^\top \begin{bmatrix}
                x_1 & x_2 & \cdots & x_N
            \end{bmatrix} -
            \begin{bmatrix}
                y_1 & y_2 & \cdots & y_N
            \end{bmatrix}
        \right)
        \begin{bmatrix}
            W^\top x_1 - y_1 \\
            W^\top x_2 - y_2 \\
            \vdots \\
            W^\top x_N - y_N
        \end{bmatrix} \\
        &= (W^\top X^\top - Y^\top)\underbrace{(X W - Y)}_{\text{左边项直接转置}} \\
        &= W^\top X^\top X W - W^\top X^\top Y - Y^\top X W + Y^\top Y
    \end{aligned}
\end{equation}
$$

于是，我们需要估计的参数$W$即为
$$
\begin{equation}
    \hat{W} = \arg \min_W L(W)
\end{equation}
$$

因为这里的目标函数$L(W)$是显式的表达式，因此可以直接求解优化目标。令$\frac{\partial L(W)}{\partial W} = 0$即可求出$W$的解析解。

这里涉及到矩阵求导，首先需要确定这是什么类型的矩阵求导。根据 $L(W)$ 的定义可知，$L(W)$ 是变量为向量的标量函数。其次需要确定矩阵导数的维度，分子为标量，分母尺寸为 $(p \times 1)$，所以分母布局的矩阵导数维度是 $(p \times 1)$，分子布局的矩阵导数维度是 $(1 \times p)$。

根据《矩阵求导（三）：向量和矩阵作为变量的标量函数》笔记中的内容，可以使用矩阵的迹和全微分作为辅助求导工具，具体求导过程如下：

$$d L(W) = d\left(W^T X^T X W - W^T X^T Y - Y^T X W + Y^T Y\right)$$

$$= \text{tr}\left(d\left(W^T X^T X W - W^T X^T Y - Y^T X W + Y^T Y\right)\right)$$

$$= \text{tr}\left(d\left(W^T X^T X W\right)\right) - \text{tr}\left(d\left(W^T X^T Y\right)\right) - \text{tr}\left(d\left(Y^T X W\right)\right)$$

$$= \text{tr}\left(d(W^T) X^T X W + W^T X^T X d(W)\right) - \text{tr}\left(d(W^T) X^T Y\right) - \text{tr}\left(Y^T X d(W)\right)$$

$$= \text{tr}\left(X^T X W d(W^T)\right) + \text{tr}\left(W^T X^T X d(W)\right) - \text{tr}\left(X^T Y d(W^T)\right) - \text{tr}\left(Y^T X d(W)\right)$$

$$= \text{tr}\left(W^T X^T X d(W)\right) + \text{tr}\left(W^T X^T X d(W)\right) - \text{tr}\left(Y^T X d(W)\right) - \text{tr}\left(Y^T X d(W)\right)$$

$$= \text{tr}\left(\left(2 W^T X^T X - 2 Y^T X\right) d(W)\right)$$

因此，分子布局的矩阵导数为：

$$\frac{\partial L(W)}{\partial W^T} = 2 W^T X^T X - 2 Y^T X$$

通过式(6)可以知道最终的矩阵导数尺寸为 $(1 \times p)$，与预期的维度一致。

于是，分母布局（梯度向量）的矩阵导数即为式(6)的转置：

$$\frac{\partial L(W)}{\partial W} = 2 X^T X W - 2 X^T Y$$

这时令矩阵导数为 0 即可求出最优解：

$$\frac{\partial L(W)}{\partial W} = 2 X^T X W - 2 X^T Y = 0$$

$$\implies X^T X W = X^T Y$$

$$\implies \hat{W} = (X^T X)^{-1} X^T Y$$

实际上，$(X^T X)^{-1} X^T$ 这部分被称为矩阵 $X$ 的伪逆，可以记作 $X^\dagger$。

# 最小二乘法的几何意义

再次回顾上面这个问题，我们有 N 个数据，需要找到一个 W，使得 f(W) 尽可能地拟合数据：

$$\begin{bmatrix} W^T x₁ \\ W^T x₂ \\ ⋮ \\ W^T x_N \end{bmatrix} = \begin{bmatrix} w₁ x₁₁ + w₂ x₁₂ + \cdots + w_p x₁p \\ w₁ x₂₁ + w₂ x₂₂ + \cdots + w_p x₂p \\ ⋮ \\ w₁ x_{N1} + w₂ x_{N2} + \cdots + w_p x_{Np} \end{bmatrix} = X W$$

也就是需要 $X W$ 与 $Y$ 尽可能地接近。

现在从几何角度看待这个问题，如果我们将 N 个 p 维数据向量看成是 p 个长度为 N 的向量，可以看作是 N 个 p 维样本构建了一个 p 维子空间：

$$X = (x₁, x₂, ⋯, x_N)^T = \begin{pmatrix} x₁₁ & x₁₂ & \cdots & x₁p \\ x₂₁ & x₂₂ & \cdots & x₂p \\ ⋮ & ⋮ & \ddots & ⋮ \\ x_{N1} & x_{N2} & \cdots & x_{Np} \end{pmatrix}_{N×p} = (V₁, V₂, ⋯, V_p)_{N×p}$$

$V_i$ 为长度为 N 的 p 维子空间的基。

由于 Y 无法完全由 $(V₁, ⋯, V_p)$ 表示（因为存在噪声，所以总是存在误差），因此 Y 并不在这个 p 维子空间中。

从几何的角度可知，与 Y 最近的 $X W$ 其实就是 Y 在这个 p 维子空间的投影，因此既然是投影，那么 $L(W) = (W^T X^T - Y^T)_{1×N}$ 是 p 维空间的法向量，换句话说 $L(W)$ 与 p 个 N 维向量都是垂直的，即：

$$(X W - Y) ⊥ X$$

因此满足如下关系：

$$X^T (X W - Y) = 0_{p×1}$$

求解上述方程可以得到：

$$W = (X^T X)^{-1} X^T Y$$

# 概率视角

$$D = \{(x₁, y₁), (x₂, y₂), ⋯, (x_N, y_N)\}$$

# 概率视角下的线性回归

## 噪声假设

假设噪声服从高斯分布：

$$\varepsilon \sim \mathcal{N}(0, \sigma^2)$$

于是需要拟合的函数 $y$ 可以表示为：

$$\begin{cases} y = f(W) + \varepsilon, & \varepsilon \sim \mathcal{N}(0, \sigma^2) \\ f(W) = W^T x \end{cases}$$

根据高斯分布的性质可得 $y | x$ 服从如下分布：

$$y | x; W \sim \mathcal{N}(W^T x, \sigma^2)$$

其中，因为 $x$ 在这里是条件，所以 $W^T x$ 可以看作常数项。

## 极大似然估计

使用极大似然估计来求解这个问题。极大似然估计的目标函数是对数似然函数：

$$L(W) = \log P(y | X; W)$$

对于独立同分布样本，可以展开为连乘形式：

$$= \log \prod_{i=1}^N P(y_i | x_i; W)$$

代入高斯分布表达式：

$$= \log \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma}} \exp\left[-\frac{(y_i - W^T x_i)^2}{2\sigma^2}\right]$$

对数连乘变为连加：

$$= \sum_{i=1}^N \log \frac{1}{\sqrt{2\pi\sigma}} - \sum_{i=1}^N \frac{(y_i - W^T x_i)^2}{2\sigma^2}$$

## 目标函数优化

最终目标是最大化对数似然函数：

$$\hat{W} = \arg \max_W L(W)$$

即：

$$= \arg \max_W \left[ - \sum_{i=1}^N \frac{(y_i - W^T x_i)^2}{2\sigma^2} \right]$$

由于 $\sigma^2$ 与 $W$ 无关，最大化等价于：

$$= \arg \min_W \sum_{i=1}^N (y_i - W^T x_i)^2$$

从上式可以看到，使用极大似然估计方法的目标函数与最小二乘法的目标函数一致。因此，在线性回归算法中，最小二乘法实际上就是噪声为高斯分布情况下的极大似然估计。

# 参考文献

[1]Bilibili《白板推导机器学习》系列

[2]C. M. Bishop. 《Pattern recognition and machine learning》
