---
title: 机器学习（四）：线性回归
tags:
---

# 线性回归

## 1. 最小二乘法（LSE）

\[ D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\} \]

\[ x_i \in \mathbb{R}^P, \quad y_i \in \mathbb{R}, \quad i = 1, 2, \cdots, N \]

\[ X = (x_1, x_2, \cdots, x_N)^T = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1P} \\ x_{21} & x_{22} & \cdots & x_{2P} \\ \vdots & \vdots & \ddots & \vdots \\ x_{N1} & x_{N2} & \cdots & x_{NP} \end{pmatrix}_{N \times P} \]

### Loss Function:

\[ L(W) = \sum_{i=1}^N \| W^T x_i - y_i \|_2^2 = \sum_{i=1}^N (W^T x_i - y_i)^2 \]

\[ = (W^T x_1 - y_1, \, W^T x_2 - y_2, \, \cdots, \, W^T x_N - y_N) \begin{pmatrix} W^T x_1 - y_1 \\ W^T x_2 - y_2 \\ \vdots \\ W^T x_N - y_N \end{pmatrix} \]

\[ = [W^T (x_1, x_2, \cdots, x_N) - (y_1, y_2, \cdots, y_N)] \begin{pmatrix} W^T x_1 - y_1 \\ W^T x_2 - y_2 \\ \vdots \\ W^T x_N - y_N \end{pmatrix} \]

\[ = (W^T X^T - Y^T)(XW - Y) = W^T X^T X W - W^T X^T Y - Y^T X W + Y^T Y \]

\[ \frac{\partial L(W)}{\partial W} = 2 X^T X W - 2 X^T Y = 0 \]

\[ \Rightarrow X^T X W = X^T Y \]

\[ \Rightarrow \hat{W} = (X^T X)^{-1} X^T Y \]

# 几何意义

N 个 p 维样本构建了 p 维子空间。

\[ X = (x_1, x_2, \cdots, x_N)^T = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1P} \\ x_{21} & x_{22} & \cdots & x_{2P} \\ \vdots & \vdots & \ddots & \vdots \\ x_{N1} & x_{N2} & \cdots & x_{NP} \end{pmatrix}_{N \times P} = (V_1, V_2, \cdots, V_P)_{N \times P} \]

\(V_i\) 为长度为 N 的 p 维子空间的基。

由于 Y 无法完全由 \((V_1, \cdots, V_P)\) 拟合，因此 Y 无法完全由 \((V_1, \cdots, V_P)\) 得到。最小二乘法得到的是 Y 在 p 维子空间的投影 \(W^T X^T\)。

投影与 \(Y^T\) 之间误差最小。

\[ W^T X^T = (w_1, w_2, \cdots, w_P)_{1 \times P} \begin{pmatrix} V_1 \\ V_2 \\ \vdots \\ V_P \end{pmatrix}_{P \times N} \]

\[ L(W) = (W^T X^T - Y^T)_{1 \times N} \]

法向量为 p 维空间的法向量。

因此：

\[ (W^T X^T - Y^T) X = 0 \quad \text{垂直（正交）} \]

\[ \Rightarrow W^T X^T X - Y^T X = 0 \]

\[ X^T X W = X^T Y \]

\[ \Rightarrow W = (X^T X)^{-1} X^T Y \]

# ⑤

## 概率视角：

\[ D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\} \]

\[ x_i \in \mathbb{R}^P, \quad y_i \in \mathbb{R}, \quad i = 1, 2, \cdots, N \]

\[
\begin{cases} 
y = f(W) + \varepsilon, & \varepsilon \sim \mathcal{N}(0, \sigma^2) \\
f(W) = W^T \alpha 
\end{cases}
\]

\[ \Rightarrow y = W^T \alpha + \varepsilon \]

因此：

\[ y | x; W \sim \mathcal{N}(W^T x, \sigma^2) \]

### 极大似然估计（MLE）：

\[ L(W) = \log P(y | X; W) = \log \prod_{i=1}^N P(y_i | x_i; W) \]

\[ = \log \prod_{i=1}^N \frac{1}{\sqrt{2\pi}\sigma} \exp\left[-\frac{(y_i - W^T x_i)^2}{2\sigma^2}\right] \]

\[ = \sum_{i=1}^N \log \frac{1}{\sqrt{2\pi}\sigma} - \sum_{i=1}^N \frac{(y_i - W^T x_i)^2}{2\sigma^2} \]

\[ \hat{W} = \arg\max_W L(W) \]

\[ = \arg\max_W \left\{ -\sum_{i=1}^N \frac{(y_i - W^T x_i)^2}{2\sigma^2} \right\} \]

\[ = \arg\min_W \sum_{i=1}^N (y_i - W^T x_i)^2 \]

因此：

\[ LSE \Leftrightarrow MLE \quad (\text{Noise is Gaussian Dist.}) \]

## 2. 正则化

### Loss Function:

\[ L(W) = \sum_{i=1}^N \| W x_i - y_i \|_2^2 \]

\[ \hat{W} = (X^T X)^{-1} X^T Y \]

\[ X_{N \times P}, \quad N \text{ 个样本}, \quad x \in \mathbb{R}^P \]

### 过拟合：

1. 加入数据
2. 特征选择/特征提取（PCA）
3. 正则化

### 正则化框架：

\[ \arg\min_W \left[ L(W) + \lambda P(W) \right] \]

- **L1**: Lasso, \( l_1 \) 范数, \( P(W) = \| W \|_1 \)
- **L2**: Ridge, \( l_2 \) 范数, 岭回归, \( P(W) = \| W \|_2^2 = W^T W \)

\[ \Rightarrow \text{weight decay 权值衰减} \]

# (1) 频率角度

\[ J(W) = L(W) + \lambda P(W) \]

\[ = \sum_{i=1}^N \| W^T x_i - y_i \|^2 + \lambda \| W \|^2 \]

\[ = \sum_{i=1}^N (W^T x_i - y_i)(W^T x_i - y_i)^T + \lambda W^T W \]

\[ = (W X^T - Y^T)(X W - Y) + \lambda W^T W \]

\[ = W^T X^T X W - 2 W^T X^T Y + Y^T Y + \lambda W^T W \]

\[ = W^T (X^T X + \lambda I) W - 2 W^T X^T Y + Y^T Y \]

\[ \hat{W} = \arg\min_W J(W) \]

\[ \frac{\partial J(W)}{\partial W} = 2 (X^T X + \lambda I) W - 2 X^T Y = 0 \]

\[ \Rightarrow \hat{W} = (X^T X + \lambda I)^{-1} X^T Y \]

\(X^T X\) 不一定可逆，\(X^T X\) 为半正定，\(X^T X + \lambda I\) 一定可逆，有效抑制了过拟合。

---

# (2) 贝叶斯角度

**先验**：

\[ W \sim \mathcal{N}(0, \sigma_0^2), \quad \varepsilon \sim \mathcal{N}(0, \sigma^2) \]

\[ P(W) = \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left\{ -\frac{\| W \|^2}{2\sigma_0^2} \right\} \]

\[ P(Y | X, W) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{ -\frac{(Y - W^T X)^2}{2\sigma^2} \right\} \]

**最大后验概率（MAP）**：

\[ \hat{W} = \arg\max_W P(W | Y) = \arg\max_W P(Y | X, W) P(W) \]

\[ = \arg\max_W \log \left[ P(Y | X, W) P(W) \right] \]

\[ = \arg\max_W \log \left[ \frac{1}{\sqrt{2\pi}\sigma} \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left\{ -\frac{(Y - W^T X)^2}{2\sigma^2} - \frac{\| W \|^2}{2\sigma_0^2} \right\} \right] \]

\[ = \arg\max_W \left[ -\frac{(Y - W^T X)^2}{2\sigma^2} - \frac{\| W \|^2}{2\sigma_0^2} \right] \]

\[ = \arg\min_W \left[ \frac{(Y - W^T X)^2}{2\sigma^2} + \frac{\| W \|^2}{2\sigma_0^2} \right] \]

\[ = \arg\min_W \left( \sum_{i=1}^N \| y_i - W^T x_i \|^2 + \frac{\sigma^2}{\sigma_0^2} \| W \|^2 \right) \]

因此：

Regularized LSE \(\Leftrightarrow\) MAP (noise is GD, prior is GD.)

---

# 总结

\[ LSE \Leftrightarrow MLE \quad \text{(noise is GD.)} \]

\[ \text{Regularized LSE} \Leftrightarrow MAP \quad \text{(noise is GD, prior is GD.)} \]