---
title: 强化学习：基础篇（一）
tags:
  - 强化学习
  - Reinforcement Learning
  - 强化学习基本概念
  - 统计学基本概念
categories:
  - AI
date: 2025-03-08 11:55:57
---


# 概述

本文主要是记录自己学习强化学习过程中的笔记，因为是笔记，所以存在不严谨的问题。

# 统计学的一些概念

在笔记开始前需要先复习一下统计学相关的基本概念。

## 随机变量(Random Variable)和观测值(Observed Value)

在统计学中有随机变量(Random Variable)和观测值(Observed Value)的概念。


!!! note 随机变量
    **随机变量**是指随机事件的结果对应的值。


比如抛硬就是一个随机事件，我们知道抛硬币过程中会产生2个结果：一个是正面朝上，一个是反面朝上。在这里我们将正面朝上记为数字1，反面朝上记为数字0。如果我们将抛硬币这个随机事件的结果记为随机变量$X$，这里通常使用大写字母来表示随机变量，$X$就有2种可能的取值：0和1。虽然在抛硬币之前我们并不知道$X$的取值，但可以知道取值的概率$\mathbb{P} (X = 0) = 0.5$和$\mathbb{P} (X = 1) = 0.5$。


!!! note 观测值
    **观测值**是随机变量的取值。


上面我们讲到抛硬币的过程会产生正面(1)或反面(0)，这实际上就是观测值。在整个地硬币的过程中(抛N次)，会有很多的观测值，也就是每次抛硬币的结果，这些观测值可以使用随机变量$X$的小写字母$x$来表示。比如抛了4次硬币，即有4个观测值，假设分别是$x_1 = 1, x_2 = 1, x_3 = 0, x_4 = 1$。这里需要注意的是观测值$x$是没有随机性的，因为已经确定这一次抛硬币的结果，而随机变量$X$才是有随机性的，因为它的取值在没有抛硬币之前并不知道，可能取值0可能取值为1。

## 概率密度函数(Probability Density Function, PDF)


!!! note 观测值
    **概率密度函数**描述的是某个随机变量的某个取值的概率。


比如正态分布(Gaussian Distribution)的概率密度函数如下:
$$
\begin{equation}
    p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right)
\end{equation}
$$
这是一个连续的概率分布，$x$的取值是实数。给定一个$x$，$p(x)$就表示随机变量取值为$x$的概率。

再比如离散随机变量$X \in \left\{1, 3, 7 \right\}$的概率密度函数如下：
$$
\begin{equation}
    \begin{aligned}
        p(1) = 0.2 \\
        p(2) = 0.5 \\
        p(7) = 0.3
    \end{aligned}
\end{equation}
$$

概率密度函数有一个非常好的性质，那就是积分为1。


!!! note 概率密度函数的性质
    积分为1。

    对于连续随机变量，$\int_x p(x) \mathrm{d} x = 1$。

    对于离散随机变量，$\sum_{x \in X} p(x) = 1$。


## 期望(Expectation)

期望有时候也被称为均值。

!!! note 期望(Expectation)
    对于连续分布，$f(X)$的期望是：
    $$
    \begin{equation}
        \mathbb{E} [f(X)] = \int_{x} p(x)f(x) \mathrm{d} x
    \end{equation}
    $$
    对于离散分布，$f(X)$的期望是：
    $$
    \begin{equation}
        \mathbb{E} [f(X)] = \sum_{x \in X} p(x)f(x)
    \end{equation}
    $$


## 随机抽样(Random Sampling)

简单来说，随机抽样就类似从一箱子的球中闭着眼睛拿出一个球的过程。每次拿出一个球就相当于产生一个观测值(比如观测值可以是球的颜色)，多次执行上面的操作之后就会产生多个观测值。有了很多观测值之后，这些观测值就具有了统计意义(比如不同颜色的球的占比与箱子中的实际占比会比较接近)。

# 强化学习

在上面的概念基础上可以开始理解强化学习中的一些概念了。

## 基础概念

我们以玩游戏为例来理解强化学习中的基础概念。

### 状态(State)、动作(Action)、智能体(Agent)

首先是 **状态(State)** $s$的概念，在玩游戏的过程中，游戏中的画面就是状态。有了状态这个概念之后，就可以根据状态给出 **动作(Action)** $a$，比如游戏角色根据游戏画面决定使用哪些动作(上、下、左、右、跳跃、攻击等等)会有利。而执行这些动作的对象被称为 **智能体(Agent)** ，相当于游戏中的角色。

这里需要注意的是，强化学习中Agent需要与环境交互，所以 **环境(Environment)** 也是强化学习中的一个基础概念。环境与状态的概念并不相同，但环境与状态也可以是相同的东西。以游戏为例，通常情况下可以将游戏画面定义为状态，游戏的运行程序定义为环境，但实际上将画面和运行程序都定义为状态也是可以的。

### 策略(Policy)

当游戏角色看到游戏画面上面有宝箱，则做出往上走的动作，目的是获取宝箱，从看到宝箱到做出往上走的动作的过程称为 **策略(Policy)** $\pi$。所以，结合上面的几个概念，策略$\pi$就是指Agent看到当前状态$s$后要给出什么样的动作$a$。从数学上来解释，Policy $\pi$ 使用一个概率密度函数来描述:
$$
\begin{equation}
    \pi (a | s) = \mathbb{P} (A = a | S = s)
\end{equation}
$$

比如在某个游戏中，角色能做的动作只有$\left\{\text{left}，\text{right}，\text{up}\right\}$，而$\pi (\text{left}|s) = 0.2$表示看到当面的游戏画面$s$后做出往左走的概率是0.2，$\pi (\text{right}|s) = 0.1$ 表示看到当面的游戏$s$画面后做出往左走的概率是0.1，$\pi (\text{up}|s) = 0.7$表示看到当面的游戏画面$s$后做出往左走的概率是0.7。当角色看到当前状态 $s$后，角色(Agent)会从这3个动作中根据这些概率进行随机抽样。从上面的描述可以看出来，不管游戏画面是什么样，所有动作都有可能被采样到，只是有些动作被采样到的概率会比较低，有些则比较高。 **从智能体策略角度来看，强化学习的根本目的实际上就是让Agent学习一个优秀的Policy，最后Agent使用学习好的Policy与环境交互** 。而对于一个游戏而言，好的策略就是能够让游戏角色通关的策略。

### 奖励(Reward)

刚才我们讲强化学习的目的就是找到一个优秀的Policy，于是就需要有一个指标来衡量Policy的好坏，这就有了 **奖励(Reward)** 的概念。在游戏中，Agent做出动作后，环境会给出相应的奖励，这个奖励有正面奖励也有负面奖励。比如角色执行向上走的动作，拿到宝箱，那么游戏就会给出正面奖励。如果角色执行向右走的动作，碰到敌人，角色受到伤害，那么游戏就会给出负面奖励。而在强化学习中，奖励通常都需要User自行设定，糟糕的奖励设定对强化学习算法的性能产生很大的影响。 **从奖励角度来看，强化学习的目标就是希望Agent拿到的奖励总和更高** 。因为我们并不知道环境/状态未来给予智能体的奖励是多少，因此奖励本身是具有随机性的，可以用大写字母$R$表示奖励这个随机变量。而已经获得的奖励已经确定，所以是观测值，用小写字母$r$表示。

### 状态转移(State Transition)

**状态转移(state Transition)** 是指当Agent执行某个Action后，Old State转换为New State的过程。比如游戏中，角色移动后游戏画面会产生变化。状态转移可以是固定的，但通常往住是随机的，因为状态的切换与环境有关，所以状态的随机性来自于环境。比如游戏过程中，游戏画面来自于游戏程序，游戏中敌人的移动可能也带有随机性。用数学描述状态转移就是如下公式：
$$
\begin{equation}
    p(s^{\prime}|s, a) = \mathbb{P}(S^{\prime} = s|S = s, A = a)
\end{equation}
$$

### 轨迹(Trajectory)

在上面讲到的状态$s$、动作$a$、奖励$r$概念上，Agent与环境交互可以产生一个“状态-动作-奖励”序列。在强化学习中将这样的序列称为Agent的 **轨迹(Trajectory)** ，它代表了智能体与环境交互的整个完整过程。
$$s_1, a_1, r_1, s_2, a_2, r_2, \cdots, s_T, a_T, r_T$$

### 回报(Returns)

上面提到的奖励是在Agent做出动作后，环境会给出的反馈，就像玩游戏的过程中每走一步会获得多少游戏积分一样。在整个Agent与环境交互的过程中会产生很多个Reward，将这些 **从当前时刻开始的奖励** 累积起来就被称为 **回报(Returns)** ，相当于一整局游戏的总得分。

!!! note 回报(Return)
    Return是指累计的未来奖励(Cumulative Future Reward)。

回报可以用下面的公式来描述：
$$
\begin{equation}
    U_t = R_t + R_{t + 1} + R_{t + 2} + R_{t + 3} + \cdots
\end{equation}
$$

在此基础上衍生出了 **折扣累计未来奖励(Cumulative Discounted Future Reward)** 的概念。这个概念的来源是基于这样一个假设：未来的奖励对当前的贡献越来越小。简单的来说就是未来的不确定性太大，未来的奖励对当前的指导性会越来越差。以游戏角度理解，如果未来离现在的时间越长，现在执行的动作可能对未来是否能赢得游戏的作用会越来越小。

!!! note 折扣回报(Discounted Return)
    Discounted Return是指累计的折扣奖励(Cumulative Discounted Future Reward)

可以使用下面的公式来描述：
$$
\begin{equation}
    U_t = R_t + \gamma R_{t + 1} + \gamma^2 R_{t + 2} + \gamma^3 R_{t + 3} + \cdots
\end{equation}
$$
其中，$\gamma$表示折扣率(Discount Rate)，这是一个超参数。

上述定义中，Reward都使用大写字母$R$表示，原因是定义中$t$时刻开始的Reward还没有被观察到，所以还是随机变量，因此用大写来表达。所以回报$U_t$同样也是随机变量。

### 价值函数(Value Function)

前面讲到**强化学习的目标就是希望Agent拿到的奖励总和更高**，实际上目标就是最大化$U_t$。由$U_t$衍生出来的价值函数有2种：动作价值函数和状态价值函数。

#### 动作价值函数(Action-Value Function)

!!! note 动作价值函数(Action-Value Function)
    Policy $\pi$ 的动作价值函数定义为:
    $$
    \begin{equation}
        Q_{\pi}(s_t, a_t) = \mathbb{E}[U_t| S_t = s_t, A_t = a_t]
    \end{equation}
    $$

因为$U_t$是随机变量，具有随机性，也就是我们并不知道它的未来会怎么样，所以这里并不是直接最大化$U_t$本身，而是最大化它的期望。通常来说，取随机变量的期望是常用的消除随机性的手段。

动作价值函数的意义就是对于当前的Policy $\pi$，衡量在当前状态$s_t$的情况下执行动作$a_t$的好坏。换句话说，给定Policy $\pi$，$Q_{\pi}$函数可以对当前状态$s_t$下的不同动作$a_t$进行打分，这样就可以知道哪个动作好，哪个动作坏了。

上面给出的动作价值函数与Policy $\pi$密切相关，不同的Policy对相同的$s_t$和$a_t$会有不同的分数。穷举所以可能的Policy就可以找到一个最优的动作价值函数，称为最优动作价值函数。

!!! note 最优动作价值函数(Optimal Action-Value Function)
    $$
    \begin{equation}
        Q^{\star} (s_t, a_t) = \max_{\pi} Q_{\pi}(s_t, a_t)
    \end{equation}
    $$

#### 状态价值函数(State-Value Function)

第二种价值函数称为状态价值函数(State-Value Function)，由动作价值函数进一步产生。
$$
\begin{equation}
    V_{\pi}(s_t) = \mathbb{E}_A [Q_{\pi} (s_t, a_t)]
\end{equation}
$$

上式中，$A$是Agent采取的动作，它来自于这样一个分布$A \sim \pi (\cdot | s_t)$。所以，如果动作空间是离散的，那么：
$$
\begin{equation}
    V_{\pi}(s_t) = \mathbb{E}_A [Q_{\pi} (s_t, a_t)] = \sum_a \pi (a | s_t) Q_{\pi} (s_t, a)
\end{equation}
$$
如果动作空间是连续的，那么：
$$
\begin{equation}
    V_{\pi}(s_t) = \mathbb{E}_A [Q_{\pi} (s_t, a_t)] = \int_a \pi (a | s_t) Q_{\pi} (s_t, a) \text{d}a
\end{equation}
$$
从上面的公式可以看到，状态价值函数是在动作价值函数的基础上对动作$A$取期望，于是随机变量$A$就会被消除。

对比动作价值函数而言，状态价值函数的意义是可以衡量当前所处的局势的好坏。比如在围棋中，$V_{\pi}$可以衡量当前牌面下Agent的胜算有多大。

最后总结一下:

1)  动作价值函数$Q_{\pi}(s_t, a_t) = \mathbb{E}[U_t| S_t = s_t, A_t = a_t]$可以衡量在给定Policy $\pi$下，当前状态为$s_t$时，执行每个动作的好坏。

2)  状态价值函数$V_{\pi}(s_t) = \mathbb{E}_A [Q_{\pi} (s_t, a_t)]$可以衡量对于一个Policy $\pi$，Agent 处在当前状态的胜算，胜算越大则状态越好。同时，$\mathbb{E}_{S} [V_{\pi} (S)]$也可以衡量Policy $\pi$的好坏。也就是说Policy越好，则$V_{\pi}$的平均值越大。

## 强化学习方法

在上面的这些基础概念之上，强化学习算法可以从以下2个方向出发：

* 方向一：既然动作$a_t$来自于分布$\pi (\cdot | s_t)$，那么就直接找出一个好的Policy $\pi (a | s)$，一旦观察到状态$s_t$，则从$a_t \sim \pi (\cdot | s_t)$中随机采样一个动作，这种方法称为Policy-based方法。
* 方向二：既然价值函数可以对状态/动作打分，那么直接找出一个最优动作价值函数$Q^{\star} (s_t, a_t)$，一旦观察到状态$s_t$，则根据动作价值函数的打分选择动作，即$a_t = \text{arg max}_a Q^{\star} (s_t, a_t)$，这种方法称为Value-based方法。

# 参考文献

笔记内容整理自《Shusen Wang, Yujun Li, and Zhihua Zhang. Deep Reinforcement Learning. Posts and Telecom Press Co., Ltd, 2022.》
