---
title: 机器学习（二）：绪论
tags:
---


# 高斯分布的边缘概率分布和条件概率分布

$p$维随机变量$X$服从高斯分布：

## 条件概率分布

③构造 \(x_{b \cdot a} = x_b - \Sigma_{ba} \Sigma_{aa}^{-1} x_a \Rightarrow x_b = x_{b \cdot a} + \Sigma_{ba} \Sigma_{aa}^{-1} x_a\)

\(x_{b \cdot a} = \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) \begin{pmatrix} x_a \\ x_b \end{pmatrix} = \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) X\)

\(\Rightarrow E(x_{b \cdot a}) = \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) \mu = \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) \begin{pmatrix} \mu_a \\ \mu_b \end{pmatrix}\)

\(= -\Sigma_{ba} \Sigma_{aa}^{-1} \mu_a + \mu_b\)

记为 \(= \mu_{b \cdot a}\)

\(D(x_{b \cdot a}) = \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) \Sigma \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right)^T\)

\(= \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) \begin{pmatrix} \Sigma_{aa} & \Sigma_{ab} \\ \Sigma_{ba} & \Sigma_{bb} \end{pmatrix} \begin{pmatrix} -(\Sigma_{aa}^{-1})^T (\Sigma_{ba})^T \\ I_n^T \end{pmatrix}\)

\(= \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) \begin{pmatrix} \Sigma_{aa} & \Sigma_{ab} \\ \Sigma_{ba} & \Sigma_{bb} \end{pmatrix} \begin{pmatrix} -\Sigma_{aa}^{-1} \Sigma_{ba} \\ I_n \end{pmatrix}\)

\(= \Sigma_{bb} - \Sigma_{ba} \Sigma_{aa}^{-1} \Sigma_{ab}\) \(\Sigma_{aa}\)在 \(\Sigma\) 中的舒尔补。

记为 \(\Sigma_{bb \cdot a}\)

**舒尔补公式：**

nxn 的矩阵写成分块形式：\(M = \left[ \begin{array}{cc|cc} A & B \\ C & D \end{array} \right]_{n \times n}\)

其中，A,D 为方阵。

- 若 A 为非奇异的，则 A 在 M 中的舒尔补为：D - C A^{-1} B

- 若 D 为非奇异的，则 D 在 M 中的舒尔补为：A - B D^{-1} C

\(\Rightarrow x_{b \cdot a} \sim N(\mu_{b \cdot a}, \Sigma_{bb \cdot a})\)

证明：\(x_{b \cdot a}\) 与 \(x_a\) 的独立性

\(x_{b \cdot a} = \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) X\) ，\(x_a = \left(I_m, 0\right) X\)

\(M \Sigma N^T = \left(-\Sigma_{ba} \Sigma_{aa}^{-1}, I_n\right) \begin{pmatrix} \Sigma_{aa} & \Sigma_{ab} \\ \Sigma_{ba} & \Sigma_{bb} \end{pmatrix} \begin{pmatrix} I_m \\ 0 \end{pmatrix} = 0\)

\(\Rightarrow x_{b \cdot a}\) 与 \(x_a\) 独立

\(\Rightarrow x_{b \cdot a}|x_a = x_{b \cdot a}\)

因此，\(x_b|x_a = x_{b \cdot a}|x_a + \Sigma_{ba} \Sigma_{aa}^{-1} x_a|x_a\)

\(= x_{b \cdot a} + \Sigma_{ba} \Sigma_{aa}^{-1} x_a\)

\(\Rightarrow E(x_b|x_a) = E(x_{b \cdot a} + \Sigma_{ba} \Sigma_{aa}^{-1} x_a)\)

\(= \mu_{b \cdot a} + \Sigma_{ba} \Sigma_{aa}^{-1} x_a\) \(\leftarrow x_{a|x_a}\)

\(D(x_b|x_a) = D(x_{b \cdot a} + \Sigma_{ba} \Sigma_{aa}^{-1} x_a)\)

\(= D(x_{b \cdot a}) = \Sigma_{bb \cdot a}\)

\(\Rightarrow x_b|x_a \sim N(\mu_{b \cdot a} + \Sigma_{ba} \Sigma_{aa}^{-1} x_a, \Sigma_{bb \cdot a})\)

同理：\(x_b \sim N(\mu_b, \Sigma_{bb})\) 线性高斯模型

\(x_a|x_b \sim N(\mu_{a \cdot b} + \Sigma_{ab} \Sigma_{bb}^{-1} x_b, \Sigma_{aa \cdot b})\)

其中，\(\mu_{a \cdot b} = -\Sigma_{ab} \Sigma_{bb}^{-1} \mu_b + \mu_a\)

\(\Sigma_{aa \cdot b} = \Sigma_{aa} - \Sigma_{ab} \Sigma_{bb}^{-1} \Sigma_{ba}\)

5. 已知：\(p(x) = N(x|\mu, \Lambda^{-1})\) ，\(\Lambda, L\) 为精度矩阵

\(p(y|x) = N(y|Ax + b, L^{-1})\) ，\(\Lambda, L^{-1}\) 为协方差矩阵

求 ① \(p(y)\) ② \(p(x|y)\)

给定一个高斯边缘分布和一个高斯条件分布，其中 \(p(x)\) 和 \(p(y|x)\) 的均值是 x 的线性函数，协方差与 x 无关，这是线性高斯模型。

\(y = Ax + b + \varepsilon\) ，\(\varepsilon \sim N(0, L^{-1})\) ，\(x \perp \varepsilon\)

① \(E[y] = E[Ax + b + \varepsilon] = E[Ax + b] + E[\varepsilon]\)

\(= A\mu + b\)

\(D[y] = D[Ax + b + \varepsilon] = D[Ax + b] + D[\varepsilon]\)

\(= \Lambda^{-1} + A \Lambda^{-1} A^T\)

\(\Rightarrow y \sim N(A\mu + b, L^{-1} + A \Lambda^{-1} A^T)\)

② \(Z = \begin{pmatrix} x \\ y \end{pmatrix} \Rightarrow E[Z] = \begin{pmatrix} \mu \\ A\mu + b \end{pmatrix}\)

\(D[x] = \Lambda^{-1}\)，\(D[y] = L^{-1} + A \Lambda^{-1} A^T\)

\(\text{cov}(x, y) = E[(x - E[x])(y - E[y])^T]\)

\(= E[(x - \mu)(Ax + b + \varepsilon - A\mu - b)^T]\)

\(= E[(x - \mu)(A(x - \mu) + \varepsilon)^T]\)

（此处公式可能有误，具体符号需要根据上下文确认）

\(= E[(x - \mu)(x - \mu)^T A^T + (x - \mu) \varepsilon^T]\)

由于 \(x \perp \varepsilon\)，后项为 0

\(= E[(x - \mu)(x - \mu)^T] A^T = D[x] A^T\)

\(= \Lambda^{-1} A^T\)


# 机器学习

机器学习领域认为，数据是一个概率模型，所以统计相关的一些技术被引入到机器学习中。

机器学习大致可以分为两个派别：频率派和贝叶斯派。

对于频率派而言，参数$\theta$被认为是未知常量，数据$\boldsymbol{X}$为随机变量。我们的目的是通过数据$\boldsymbol{X}$将参数$\theta$估计出来，常用的估计方法是极大似然估计。
对于贝叶斯派而言，参数$\theta$也被认为是随机变量，并且$\theta \sim p(\theta)$，这里$p(\theta)$被称为先验。在实际建模的过程中，通过贝叶斯公式将先验与似然联系起来，即
$$
\begin{equation}
    \underbrace{P(\theta | \boldsymbol{X})}_{\text{Posterior}} = \frac{\overbrace{P(\boldsymbol{X} | \theta)}^{\text{Likelihood}} \overbrace{P(\theta)}^{\text{Prior}}}{\underbrace{P(\boldsymbol{X})}_{\int_{\theta} P(\boldsymbol{X} | \theta) P(\theta) \text{d} \theta}}
\end{equation}
$$

# 参考文献

[1]Bilibili《白板推导机器学习》系列

[2]C. M. Bishop. 《Pattern recognition and machine learning》